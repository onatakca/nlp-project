{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: TF-IDF vs Word2Vec vs BERT\n",
    "\n",
    "This notebook compares all three approaches for genre classification:\n",
    "1. **TF-IDF + Logistic Regression** - Keyword-based baseline\n",
    "2. **Word2Vec + Logistic Regression** - Semantic embeddings\n",
    "3. **BERT/DistilBERT** - Transformer-based contextual understanding\n",
    "\n",
    "All models use the same train/test split for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data_loader import load_and_prepare_data\n",
    "from src.models import TFIDFModel, Word2VecModel, BERTModel\n",
    "from src.evaluate import (\n",
    "    evaluate_model, \n",
    "    plot_confusion_matrix, \n",
    "    plot_per_genre_metrics,\n",
    "    compare_models,\n",
    "    create_results_table\n",
    ")\n",
    "from src.utils import set_seed, get_gpu_info\n",
    "\n",
    "print(\"✓ Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "set_seed(RANDOM_STATE)\n",
    "\n",
    "# Data parameters\n",
    "SAMPLES_PER_GENRE = 20000  # Adjust based on available time/resources\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Model flags - set to False to skip training\n",
    "TRAIN_TFIDF = True\n",
    "TRAIN_WORD2VEC = True\n",
    "TRAIN_BERT = True  # Set to False if no GPU available\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Samples per genre: {SAMPLES_PER_GENRE:,}\")\n",
    "print(f\"  Test size: {TEST_SIZE}\")\n",
    "print(f\"  Random state: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability (for BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = get_gpu_info()\n",
    "print(f\"CUDA available: {gpu_info['cuda_available']}\")\n",
    "print(f\"Number of GPUs: {gpu_info['device_count']}\")\n",
    "\n",
    "for gpu in gpu_info['devices']:\n",
    "    print(f\"  GPU {gpu['id']}: {gpu['name']} ({gpu['memory_total']:.1f} GB)\")\n",
    "\n",
    "if not gpu_info['cuda_available'] and TRAIN_BERT:\n",
    "    print(\"\\n⚠️  No GPU detected. BERT training will be very slow on CPU.\")\n",
    "    print(\"Consider setting TRAIN_BERT = False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load once and use for all models to ensure fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_and_prepare_data(\n",
    "    samples_per_genre=SAMPLES_PER_GENRE,\n",
    "    test_size=TEST_SIZE,\n",
    "    use_cached=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Genres: {sorted(y_train.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TFIDF:\n",
    "    print(\"=\"*70)\n",
    "    print(\"TRAINING TF-IDF MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    tfidf_model = TFIDFModel(\n",
    "        classifier_type='logistic',\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=5,\n",
    "        max_df=0.8\n",
    "    )\n",
    "    \n",
    "    tfidf_model.fit(X_train, y_train)\n",
    "    y_pred_tfidf = tfidf_model.predict(X_test)\n",
    "    \n",
    "    tfidf_time = time.time() - start_time\n",
    "    results_tfidf = evaluate_model(y_test, y_pred_tfidf, model_name=\"TF-IDF\")\n",
    "    \n",
    "    print(f\"\\n✓ TF-IDF trained in {tfidf_time:.1f}s\")\n",
    "    print(f\"Accuracy: {results_tfidf['accuracy']:.4f}\")\n",
    "    print(f\"Macro F1: {results_tfidf['macro_avg']['f1']:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping TF-IDF training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_WORD2VEC:\n",
    "    print(\"=\"*70)\n",
    "    print(\"TRAINING WORD2VEC MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    w2v_model = Word2VecModel(\n",
    "        classifier_type='logistic',\n",
    "        vector_size=200,\n",
    "        window=5,\n",
    "        min_count=5,\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    w2v_model.fit(X_train, y_train)\n",
    "    y_pred_w2v = w2v_model.predict(X_test)\n",
    "    \n",
    "    w2v_time = time.time() - start_time\n",
    "    results_w2v = evaluate_model(y_test, y_pred_w2v, model_name=\"Word2Vec\")\n",
    "    \n",
    "    print(f\"\\n✓ Word2Vec trained in {w2v_time:.1f}s\")\n",
    "    print(f\"Accuracy: {results_w2v['accuracy']:.4f}\")\n",
    "    print(f\"Macro F1: {results_w2v['macro_avg']['f1']:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping Word2Vec training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BERT Model\n",
    "\n",
    "**Note:** This will take significantly longer (~50-60 minutes with 20K samples/genre on 8xA16 GPUs)\n",
    "\n",
    "You can:\n",
    "- Reduce `SAMPLES_PER_GENRE` for faster testing\n",
    "- Set `TRAIN_BERT = False` above to skip\n",
    "- Or use the command-line script instead: `python scripts/train.py --config experiments/configs/bert_config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_BERT:\n",
    "    print(\"=\"*70)\n",
    "    print(\"TRAINING BERT MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    bert_model = BERTModel(\n",
    "        model_name='distilbert-base-multilingual-cased',\n",
    "        max_length=256,  # Optimized for speed\n",
    "        batch_size=96,   # Total batch size across all GPUs\n",
    "        learning_rate=2e-5,\n",
    "        epochs=5,        # Early stopping recommended\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.1,  # Stronger regularization to prevent overfitting\n",
    "        use_amp=True     # Mixed precision for 2x speedup\n",
    "    )\n",
    "    \n",
    "    bert_model.fit(X_train, y_train)\n",
    "    y_pred_bert = bert_model.predict(X_test)\n",
    "    \n",
    "    bert_time = time.time() - start_time\n",
    "    results_bert = evaluate_model(y_test, y_pred_bert, model_name=\"BERT\")\n",
    "    \n",
    "    print(f\"\\n✓ BERT trained in {bert_time/60:.1f} minutes\")\n",
    "    print(f\"Accuracy: {results_bert['accuracy']:.4f}\")\n",
    "    print(f\"Macro F1: {results_bert['macro_avg']['f1']:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping BERT training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results from trained models\n",
    "results_list = []\n",
    "if TRAIN_TFIDF:\n",
    "    results_list.append(results_tfidf)\n",
    "if TRAIN_WORD2VEC:\n",
    "    results_list.append(results_w2v)\n",
    "if TRAIN_BERT:\n",
    "    results_list.append(results_bert)\n",
    "\n",
    "# Create comparison table\n",
    "if results_list:\n",
    "    comparison_df = create_results_table(results_list)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No models trained - enable at least one model above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_list:\n",
    "    # Compare F1 scores across genres\n",
    "    compare_models(\n",
    "        results_list,\n",
    "        metric='f1',\n",
    "        title=\"F1 Score Comparison by Genre\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_list:\n",
    "    # Compare precision across genres\n",
    "    compare_models(\n",
    "        results_list,\n",
    "        metric='precision',\n",
    "        title=\"Precision Comparison by Genre\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_TFIDF:\n",
    "    plot_confusion_matrix(\n",
    "        y_test, \n",
    "        y_pred_tfidf,\n",
    "        title=\"TF-IDF Confusion Matrix\",\n",
    "        normalize=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_WORD2VEC:\n",
    "    plot_confusion_matrix(\n",
    "        y_test, \n",
    "        y_pred_w2v,\n",
    "        title=\"Word2Vec Confusion Matrix\",\n",
    "        normalize=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_BERT:\n",
    "    plot_confusion_matrix(\n",
    "        y_test, \n",
    "        y_pred_bert,\n",
    "        title=\"BERT Confusion Matrix\",\n",
    "        normalize=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "**Expected performance (based on previous experiments):**\n",
    "\n",
    "1. **TF-IDF (~61% accuracy)**\n",
    "   - Fast training (< 1 minute)\n",
    "   - Good baseline for keyword-based features\n",
    "   - Best for: country, rap, rock (distinctive vocabulary)\n",
    "   - Worst for: pop (generic language)\n",
    "\n",
    "2. **Word2Vec (~56% accuracy)**\n",
    "   - Medium training time (2-3 minutes)\n",
    "   - Surprisingly underperforms TF-IDF\n",
    "   - May benefit from larger corpus or pre-trained embeddings\n",
    "\n",
    "3. **BERT (~61-63% accuracy)**\n",
    "   - Slow training (50-60 minutes with 20K samples/genre)\n",
    "   - Similar to TF-IDF but with better contextual understanding\n",
    "   - Prone to overfitting after epoch 3 (use early stopping!)\n",
    "   - Best overall but marginal improvement may not justify cost\n",
    "\n",
    "**Common challenges:**\n",
    "- Pop genre is consistently hardest to classify across all models\n",
    "- Rap/R&B confusion due to overlapping themes\n",
    "- Country performs best (most distinctive features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Error Analysis**: Examine misclassified examples to understand model limitations\n",
    "2. **Hyperparameter Tuning**: Use the config files in `experiments/configs/` to experiment\n",
    "3. **Feature Engineering**: Add metadata (artist, year) to improve predictions\n",
    "4. **Ensemble Methods**: Combine TF-IDF and BERT predictions\n",
    "5. **Publication**: Use the command-line scripts for reproducible experiments:\n",
    "   ```bash\n",
    "   python scripts/train.py --config experiments/configs/tfidf_config.yaml\n",
    "   python scripts/train.py --config experiments/configs/word2vec_config.yaml\n",
    "   python scripts/train.py --config experiments/configs/bert_config.yaml\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
