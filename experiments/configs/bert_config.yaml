# BERT/DistilBERT Configuration

model:
  type: "bert"
  model_name: "distilbert-base-multilingual-cased"  # or 'bert-base-multilingual-cased'

  # Training parameters
  max_length: 256
  batch_size: 96  # Total batch size (will be distributed across GPUs)
  learning_rate: 0.00002  # 2e-5
  epochs: 5
  warmup_ratio: 0.1
  weight_decay: 0.1
  use_amp: true  # Automatic Mixed Precision

data:
  samples_per_genre: 20000  # Can increase to 50000 for better results
  test_size: 0.2
  random_state: 42
  use_cached: true

experiment:
  name: "bert_optimized"
  description: "DistilBERT with optimizations (multi-GPU, FP16)"
  save_results: true
